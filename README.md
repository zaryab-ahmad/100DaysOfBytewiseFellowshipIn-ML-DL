# 100DaysOfBytewiseFellowshipIn-ML-DL
I'll continue updating this readme file as I upload more tasks. I'm grateful to be part of this fellowship, where I've found a conducive environment to work and a well-structured roadmap for learning ML (Machine Learning) and DL (Deep Learning). Many thanks to the Bytewise team and our lead for their support and guidance.

Learning from the First Week of the Fellowship

In the inaugural week of the Bytewise Fellowship, I embarked on a comprehensive journey spanning foundational Python skills to advanced algorithms. Here’s a recap of my key learnings:

Python Proficiency:

Strengthened foundational understanding of Python, encompassing syntax, user input/output, type casting, control flow, and manipulation of lists and strings.
Gained confidence in basic coding practices, laying a robust groundwork for more complex tasks ahead.
Intermediate Python Challenges:

Tackled intermediate-level problems such as palindrome identification, Fibonacci sequence generation, prime number detection, and temperature conversions.
Enhanced problem-solving skills and familiarity with advanced Python features, particularly in handling strings and numerical operations.
Data Structures and Algorithms (DSA):

Delved into fundamental DSA concepts including dynamic programming and applications like Binary Search Trees, merge operations, and palindrome detection in linked lists.
Advanced understanding of algorithmic thinking, breaking down complex problems into manageable parts.
Advanced DSA Topics:

Explored advanced DSA topics like Quick Sort, Knapsack problem solutions, graph traversals (DFS & BFS), Dijkstra’s Algorithm, and finding the Longest Common Subsequence.
Developed proficiency in implementing efficient algorithms and further honed problem-solving methodologies.
Overall Experience:

Found the fellowship environment conducive to learning, with a structured roadmap for progressing through ML/DL concepts.
Grateful to the Bytewise team and our lead for their support and guidance throughout this enriching journey.
This first week provided a solid foundation and set the stage for deeper exploration and mastery in the weeks to come.

Week 2 Learning Summary
In Week 2, the focus was on learning and utilizing various Python libraries for data analysis, manipulation, and machine learning. These libraries included NumPy, Pandas, Matplotlib, Seaborn, and Scikit-learn. We applied our learning to several datasets, performing preprocessing, visualization, and model training tasks. Below is a detailed summary of the concepts and tasks covered during this week.

Libraries Covered
NumPy
Purpose: Numerical computing with powerful n-dimensional array objects.
Key Functions: Array creation, mathematical operations, and broadcasting.
Pandas
Purpose: Data manipulation and analysis.
Key Functions: DataFrame creation, data cleaning, and aggregation.
Matplotlib
Purpose: Data visualization.
Key Functions: Creating line plots, bar charts, scatter plots, and subplots.
Seaborn
Purpose: Statistical data visualization built on top of Matplotlib.
Key Functions: Creating pairplots, box plots, heatmaps, and violin plots.
Scikit-learn
Purpose: Machine learning library for model training and evaluation.
Key Functions: Model training, validation, and evaluation using various algorithms.
Datasets Worked On
Titanic Dataset: Passenger data from the Titanic disaster.
Iris Dataset: Measurements of iris flowers.
Red and White Wine Datasets: Chemical properties of red and white wines.
Bike Sharing Dataset: Data on bike rentals over time.
Heart Disease Dataset: Medical data for predicting heart disease.
Additional Datasets: Various other datasets for practice and exploration.
Key Concepts and Tasks
Data Preprocessing
Handling Missing Values: Techniques such as imputation and dropping missing data.
Dropping Unwanted Columns: Removing irrelevant or redundant features.
Feature Engineering: Creating new features from existing data to improve model performance.
Data Visualization
Plotting with Matplotlib: Creating basic plots to visualize data trends and distributions.
Advanced Visualizations with Seaborn: Using Seaborn for more complex and aesthetically pleasing visualizations.
Model Training
Logistic Regression: Training and evaluating logistic regression models for classification tasks.
Linear Regression: Implementing linear regression for predicting continuous outcomes.
Tree-Based Models: Utilizing decision trees for classification and regression tasks.
Workflow
Loading Data: Reading datasets into Pandas DataFrames.
Data Cleaning: Handling missing values, removing unwanted columns, and standardizing data.
Exploratory Data Analysis (EDA): Using visualizations to understand data distributions and relationships.
Feature Engineering: Creating and selecting the most relevant features for model training.
Model Training and Evaluation: Implementing machine learning algorithms, training models, and evaluating their performance using metrics such as accuracy, precision, recall, and F1 score.

